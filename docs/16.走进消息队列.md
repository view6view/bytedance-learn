# 介绍

![image-20220531101727498](images/image-20220531101727498.png)

## 案例一：系统崩溃

如果此时记录存储程序所在的机房被删库跑路了，上面这个流程会发生什么问题？

![image-20220531101851689](images/image-20220531101851689.png)

> 解决方案：解耦

![image-20220531102721190](images/image-20220531102721190.png)

## 案例二：秒杀，服务量能力优先

面对庞大的请求量，处理订单的服务只能同时处理10订单请求

![image-20220531102111861](images/image-20220531102111861.png)

> 解决方案：削峰

![image-20220531103050137](images/image-20220531103050137.png)

## 案例三：链路耗时长尾

对于这个流程应该怎么优化来挽回用户？

![image-20220531102246206](images/image-20220531102246206.png)

> 解决方案：异步

![image-20220531103213373](images/image-20220531103213373.png)

## 案例四：日志存储

![image-20220531103242760](images/image-20220531103242760.png)

## 什么是消息队列？

消息队列（MQ），指保存消息的一个容器，本质是一个队列。但这个队列，需要支持高吞吐、高并发，并且高可用。

![image-20220531103438472](images/image-20220531103438472.png)

# 前世今生

## 消息队列发展历程

![image-20220531103715825](images/image-20220531103715825.png)

## 业界消息队列对比

- Kafka：分布式的、分区的、多副本的日志提交服务，在**高吞吐场景下**发挥比较出色
- RocketMQ：低延迟、强一致、高性能、高可靠、万亿级容量和灵活的可扩展性，在一些**实时场景中**运用较广
- Pulsar：下一代云原生分布式消息流平台 ，集消息、存储、轻量化函数式计算为一体、采用存算分离的架构设计
- BMQ：和Pulsar架构类似，存算分离，初期定位是承接高吞吐的离线业务场景，逐步替换掉对于的Kafka集群

# Kafka

## 使用场景

- 搜索服务
- 直播服务
- 订单服务
- 支付服务

![image-20220531104355701](images/image-20220531104355701.png)

## 如何使用Kafka

- 创建集群
- 新增Topic
- 编写生产者逻辑
- 编写消费者逻辑

## 基本概念

![image-20220531110003752](images/image-20220531110003752.png)

- Topic：逻辑队列，不同Topic可以建立不同的Topic
- Cluster：物理集群，每个集群可以建立多个不同的Topic
- Producer：生产者，负责将业务消息发送到topic中
- Consumer：消费者，负责消费Topic中的信息
- ConsumerGroup：消费者组，不同组Consumer消费进度互不干涉

### Offset

消息在 partition 内的相对位置信息，可以理解为唯一ID，在 partition 内部严格递增

![image-20220531122901505](images/image-20220531122901505.png)

### Replica

每个分片有多个 Replica，Leader Replica 将会从 ISR 中选出

![image-20220531123047527](images/image-20220531123047527.png)

### 数据复制

![image-20220531123629854](images/image-20220531123629854.png)

### Kafka架构

Zookeeper：负责存储集群元信息，包括分区分配信息等

![image-20220531124017231](images/image-20220531124017231.png)

## Producer-数据发送

> 从一条消息角度看，为什么 Kafka 能支持这么高的吞吐？

![image-20220531124345312](images/image-20220531124345312.png)

> 如果发送一条消息，等到其成功之后再发一条会有什么问题？

![image-20220531124455260](images/image-20220531124455260.png)

### 批量发送

![image-20220531124611636](images/image-20220531124611636.png)

思考：如果消息量很大，网络带宽不够用，如何解决？

### 数据压缩

通过压缩，减少消息大小，目前支持Snappy、Gzip、LZ4、ZSTD压缩算法

![image-20220531130902497](images/image-20220531130902497.png)

## Broker-数据的存储

如何存储到磁盘？

![image-20220531131103889](images/image-20220531131103889.png)

### 消息文件结构

数据路径：`/Topic/Partition/Segment/(log | index | timeindex | ...)`

![image-20220531131203406](images/image-20220531131203406.png)

### 磁盘结构

移动磁头找到对应磁道，磁盘转动，找到对应扇区，最后写入。寻道成本比较高，因此顺序写可以减少寻道所带来的时间成本。

![image-20220531131557922](images/image-20220531131557922.png)

### 顺序写

采用顺序写的方式进行写入，以提高写入效率

![image-20220531131702647](images/image-20220531131702647.png)

### 如何找到消息

Consumer 通过发送 FetchRequest 请求消息数据，Broker 会将指定 Offset 处的消息，按照时间窗口和消息大小窗口发送 Consumer，**寻找数据这个细节是如何做到的呢？**

![image-20220531131942078](images/image-20220531131942078.png)

### 偏移量索引文件

目标：寻找 offset = 28

![image-20220531132404039](images/image-20220531132404039.png)

二分找到小于目标 offset 的最大索引位置

![image-20220531132458243](images/image-20220531132458243.png)

### 时间戳索引文件

二分找到小于目标时间戳最大的索引位置，再通过寻找 offset 的方式找到最终数据。

![image-20220531132822978](images/image-20220531132822978.png)

### 传统数据拷贝

![image-20220531132946730](images/image-20220531132946730.png)

### 零拷贝

![image-20220531133026756](images/image-20220531133026756.png)

## Consumer-消息的接收端

如何解决 Partition 在 Consumer Group 中分配的问题？

![image-20220531133146838](images/image-20220531133146838.png)

### Low Level

通过手动进行分配，哪一个 Consumer 消费哪一个 Partition 完全由业务来决定。

![image-20220531150004925](images/image-20220531150004925.png)

思考一下，这种方式的缺点是什么？

- 没有容灾机制，当某个Consumer挂掉的时候，消息得不到及时的处理。
- 数据中断的问题，新增消费者的时候，需要停止消费

### High Level

![image-20220531150349711](images/image-20220531150349711.png)

### Rebalance

![image-20220531150505169](images/image-20220531150505169.png)

## 数据复制问题

![image-20220531150650296](images/image-20220531150650296.png)

## 重启操作

![image-20220531150953170](images/image-20220531150953170.png)

不能够并发的重启，如果当某个分片副本都在两台机器上时候，并且对这两台机器进行重启，那么就会出现分片不可用的状态

## 替换、扩容、缩容

思考一下，替换、扩容、缩容的流程应该是怎么样的？

![image-20220531151620016](images/image-20220531151620016.png)

## 负载不均衡

![image-20220531151834754](images/image-20220531151834754.png)

![image-20220531151847894](images/image-20220531151847894.png)

# BMQ

## 简介

兼容 Kafka 协议，存算分离，云原生消息队列

> 架构图

![image-20220531152442199](images/image-20220531152442199.png)

## 运维操作对比

![image-20220531152652669](images/image-20220531152652669.png)

## HDFS写文件流程

随机选择一定数量的 DataNode 进行写入

![image-20220531152822736](images/image-20220531152822736.png)

## BMQ 文件结构

在BMQ中，每个 Segment 都随机的选择三个节点进行写入，让同一个 Partition 的所有 Segment 不至于像Kafka一样集中分配，而是打散分配

![image-20220531153007552](images/image-20220531153007552.png)

## Broker-Partition 状态机

保证对于任意分片在同一时刻只能在一个 Broker 上存活，简单来说就是一个文件必须保证一个进程进行写入

![image-20220531153752883](images/image-20220531153752883.png)

## Broker-写文件流程

![image-20220531153856884](images/image-20220531153856884.png)

## Broker-写文件 Failover

如果 DataNode 节点挂了或者是其他原因导致我们写文件失败，应该如何处理？

更换一个新的可用的节点，新增一个 Segment 进行写入，保证写入操作不会被中断掉。

![image-20220531154440584](images/image-20220531154440584.png)

## Proxy

![image-20220531154656893](images/image-20220531154656893.png)

## 多机房部署

![image-20220531154937056](images/image-20220531154937056.png)

## 高级特性

![image-20220531155227029](images/image-20220531155227029.png)

### 泳道消息

![image-20220531155421017](images/image-20220531155421017.png)

BOE：Bytedance Office Environment，是一套完全独立的线下机房环境

PPE：Product Preview Environment，即产品预览环境

> BOE测试

![image-20220531155619381](images/image-20220531155619381.png)

多个人测试，需要等待上一个人测试完成

![image-20220531160334372](images/image-20220531160334372.png)

每多一个测试人员，都需要重新搭建一个相同配置的Topic，造成人力和资源的浪费

> PPE 验证

![image-20220531160626229](images/image-20220531160626229.png)

对于 PPE 的消费者来说，资源资源没有生成环境多，所以无法承受生产环境的流量

![image-20220531160733911](images/image-20220531160733911.png)

解决主干泳道流量隔离问题以及泳道资源重复创建问题

### Databus

![image-20220531161350454](images/image-20220531161350454.png)

直接使用原生 SDK 会有什么问题？

- 客户端配置较为复杂
- 不支持动态配置，更改配置需要停掉服务
- 对于 latency 不是很敏感的业务，batch效果不佳

![image-20220531161427387](images/image-20220531161427387.png)

- 简化消息队列客户端复杂度
- 解耦业务与 Topic
- 缓解集群压力，提高吞吐

### Mirror

思考一下，是否可用通过多机房部署的方式，解决跨 Region 读写的问题？

![image-20220531162247888](images/image-20220531162247888.png)

使用 Mirror 通过最终一致的方式，解决跨 Region 读写问题

![image-20220531162423913](images/image-20220531162423913.png)

### Index

如果希望通过写入的 LogId、UserId 或者其他的业务字段进行消息的查询，应该怎么做？

![image-20220531162650625](images/image-20220531162650625.png)

直接在 BMQ 中将数据结构化，配置索引 DDL，异步构建索引后，通过 Index Query 服务读取处数据

![image-20220531162811285](images/image-20220531162811285.png)

### Parquet

Apache Parque是Hadoop生态圈中一种新型列式存储格式，它可以兼容 Hadoop 生态圈中大多数计算框架（Hadoop、Spark等），被多种查询引擎支持（Hive、Impala、Drill等）。

![image-20220531163111918](images/image-20220531163111918.png)

直接将 BMQ 中数据结构化，通过 Parquet Engine，可用使用不同的方式构建 Parquet 格式文件。

![image-20220531163238486](images/image-20220531163238486.png)

# RocketMQ

## 使用场景

例如，针对电商业务线，其业务涉及广泛，如注册、订单、库存、物流等；同时，也会涉及许多业务峰值时刻，如秒杀活动、周年庆、定期特惠等。

## 基本概念

![image-20220531163552884](images/image-20220531163552884.png)

![image-20220531163704224](images/image-20220531163704224.png)

## RocketMQ 架构

![image-20220531163852679](images/image-20220531163852679.png)

## 存储模型

![image-20220531163956840](images/image-20220531163956840.png)

## 高级特性

### 事务场景

![image-20220531164133615](images/image-20220531164133615.png)

**两阶段提交**

![image-20220531164352774](images/image-20220531164352774.png)

### 延迟发送

![image-20220531164501778](images/image-20220531164501778.png)

![image-20220531164547298](images/image-20220531164547298.png)

### 处理失败

如何处理失败的消息呢？

![image-20220531164723355](images/image-20220531164723355.png)

### 消费重试和死信队列

![image-20220531164804979](images/image-20220531164804979.png)

*最下方yes箭头指向的是死信队列